{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to tensorflow\n",
    "\n",
    "## Include library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare tensors to use as variable and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"x_3:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(35, name='x')\n",
    "y = tf.Variable(x + 5, name='y')\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is that???\n",
    "\n",
    "Not so fast my friend! The problem here is that a tensor is not a regular variable, it needs special care and use.<p> <p>\n",
    "\n",
    "Tensorflow will always require to start a session! Why? Because is the way it can reserve the memory address space to optimize the calculation. Once inside the session, you can check your variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "# Create the model initializing all the variables you already defined previously! \n",
    "# A graph is created of the dependencies between the variable\n",
    "model = tf.global_variables_initializer()\n",
    "\n",
    "#Get in the session\n",
    "with tf.Session() as session:\n",
    "    # Run the session model created\n",
    "    session.run(model)\n",
    "    # Check results\n",
    "    print(session.run(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is it possible to work with other data types?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** List (NDArray)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40 45 50]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([35, 40, 45], name='x')\n",
    "y = tf.Variable(x + 5, name='y')\n",
    "\n",
    "\n",
    "model = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(model)\n",
    "    print(session.run(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Numpy Arrays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[193 387 730 ... 248 210 875]\n",
      "[ 185681  747699 2662325 ...  306791  219885 3825515]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data = np.random.randint(1000, size=10000)\n",
    "\n",
    "x = tf.constant(data, name='x')\n",
    "y = tf.Variable(5*np.square(x) - 3*x + 15, name='y')\n",
    "\n",
    "\n",
    "model = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(model)\n",
    "    print(session.run(x))\n",
    "    print(session.run(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Operating two Matrices **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_14:0\", shape=(2, 3), dtype=int32)\n",
      "[[ 2  4  6]\n",
      " [ 8 10 12]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[1, 2, 3], [4, 5, 6]], name='a')\n",
    "b = tf.constant([[1, 2, 3], [4, 5, 6]], name='b')\n",
    "add_op = a + b\n",
    "print(add_op)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    print(session.run(add_op))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Matrix - Scalar add**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101 102 103]\n",
      " [104 105 106]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[1, 2, 3], [4, 5, 6]], name='a')\n",
    "b = tf.constant(100, name='b')\n",
    "add_op = a + b\n",
    "\n",
    "with tf.Session() as session:\n",
    "    print(session.run(add_op))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are the dimensions important?\n",
    "On tensorflow is actually the most important thing you take in consideration! Let's suppose the adding of a 3x2 matrix with a 2 element vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101 103 105]\n",
      " [104 106 108]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[1, 2, 3], [4, 5, 6]], name='a')\n",
    "b = tf.constant([100, 101, 102], name='b')\n",
    "add_op = a + b\n",
    "\n",
    "with tf.Session() as session:\n",
    "    print(session.run(add_op))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is not as hard, but can take a while to notice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(2), Dimension(3)])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(3)])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101 102 103]\n",
      " [105 106 107]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[1, 2, 3], [4, 5, 6]], name='a')\n",
    "b = tf.constant([[100], [101]], name='b')\n",
    "add_op = a + b\n",
    "\n",
    "with tf.Session() as session:\n",
    "    print(session.run(add_op))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a deep look inside!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(2), Dimension(3)])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(2), Dimension(1)])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get to work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mnist!\n",
    "A standard benchmark for neural network classification is the MNIST digits dataset, a set of 100,000 28×28 images of hand-written digits. Each MNIST digit is labeled with the correct digit class (0, 1, ... 9).\n",
    "\n",
    "The first (“input”) and last (“output”) layers in your network must match the size of the data you’ll be providing. For an MNIST classification task, this means your network must have 784 inputs (one for each image pixel) and 10 outputs (one for each class).\n",
    "\n",
    "<img src=\"mnist.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.contrib.learn.python.learn.datasets.base.Datasets"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class InteractiveSession in module tensorflow.python.client.session:\n",
      "\n",
      "class InteractiveSession(BaseSession)\n",
      " |  A TensorFlow `Session` for use in interactive contexts, such as a shell.\n",
      " |  \n",
      " |  The only difference with a regular `Session` is that an `InteractiveSession`\n",
      " |  installs itself as the default session on construction.\n",
      " |  The methods `tf.Tensor.eval`\n",
      " |  and `tf.Operation.run`\n",
      " |  will use that session to run ops.\n",
      " |  \n",
      " |  This is convenient in interactive shells and [IPython\n",
      " |  notebooks](http://ipython.org), as it avoids having to pass an explicit\n",
      " |  `Session` object to run ops.\n",
      " |  \n",
      " |  For example:\n",
      " |  \n",
      " |  ```python\n",
      " |  sess = tf.InteractiveSession()\n",
      " |  a = tf.constant(5.0)\n",
      " |  b = tf.constant(6.0)\n",
      " |  c = a * b\n",
      " |  # We can just use 'c.eval()' without passing 'sess'\n",
      " |  print(c.eval())\n",
      " |  sess.close()\n",
      " |  ```\n",
      " |  \n",
      " |  Note that a regular session installs itself as the default session when it\n",
      " |  is created in a `with` statement.  The common usage in non-interactive\n",
      " |  programs is to follow that pattern:\n",
      " |  \n",
      " |  ```python\n",
      " |  a = tf.constant(5.0)\n",
      " |  b = tf.constant(6.0)\n",
      " |  c = a * b\n",
      " |  with tf.Session():\n",
      " |    # We can also use 'c.eval()' here.\n",
      " |    print(c.eval())\n",
      " |  ```\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      InteractiveSession\n",
      " |      BaseSession\n",
      " |      SessionInterface\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, target='', graph=None, config=None)\n",
      " |      Creates a new interactive TensorFlow session.\n",
      " |      \n",
      " |      If no `graph` argument is specified when constructing the session,\n",
      " |      the default graph will be launched in the session. If you are\n",
      " |      using more than one graph (created with `tf.Graph()` in the same\n",
      " |      process, you will have to use different sessions for each graph,\n",
      " |      but each graph can be used in multiple sessions. In this case, it\n",
      " |      is often clearer to pass the graph to be launched explicitly to\n",
      " |      the session constructor.\n",
      " |      \n",
      " |      Args:\n",
      " |        target: (Optional.) The execution engine to connect to.\n",
      " |          Defaults to using an in-process engine.\n",
      " |        graph: (Optional.) The `Graph` to be launched (described above).\n",
      " |        config: (Optional) `ConfigProto` proto used to configure the session.\n",
      " |  \n",
      " |  close(self)\n",
      " |      Closes an `InteractiveSession`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseSession:\n",
      " |  \n",
      " |  __del__(self)\n",
      " |  \n",
      " |  as_default(self)\n",
      " |      Returns a context manager that makes this object the default session.\n",
      " |      \n",
      " |      Use with the `with` keyword to specify that calls to\n",
      " |      `tf.Operation.run` or `tf.Tensor.eval` should be executed in\n",
      " |      this session.\n",
      " |      \n",
      " |      ```python\n",
      " |      c = tf.constant(..)\n",
      " |      sess = tf.Session()\n",
      " |      \n",
      " |      with sess.as_default():\n",
      " |        assert tf.get_default_session() is sess\n",
      " |        print(c.eval())\n",
      " |      ```\n",
      " |      \n",
      " |      To get the current default session, use `tf.get_default_session`.\n",
      " |      \n",
      " |      *N.B.* The `as_default` context manager *does not* close the\n",
      " |      session when you exit the context, and you must close the session\n",
      " |      explicitly.\n",
      " |      \n",
      " |      ```python\n",
      " |      c = tf.constant(...)\n",
      " |      sess = tf.Session()\n",
      " |      with sess.as_default():\n",
      " |        print(c.eval())\n",
      " |      # ...\n",
      " |      with sess.as_default():\n",
      " |        print(c.eval())\n",
      " |      \n",
      " |      sess.close()\n",
      " |      ```\n",
      " |      \n",
      " |      Alternatively, you can use `with tf.Session():` to create a\n",
      " |      session that is automatically closed on exiting the context,\n",
      " |      including when an uncaught exception is raised.\n",
      " |      \n",
      " |      *N.B.* The default session is a property of the current thread. If you\n",
      " |      create a new thread, and wish to use the default session in that\n",
      " |      thread, you must explicitly add a `with sess.as_default():` in that\n",
      " |      thread's function.\n",
      " |      \n",
      " |      *N.B.* Entering a `with sess.as_default():` block does not affect\n",
      " |      the current default graph. If you are using multiple graphs, and\n",
      " |      `sess.graph` is different from the value of `tf.get_default_graph`,\n",
      " |      you must explicitly enter a `with sess.graph.as_default():` block\n",
      " |      to make `sess.graph` the default graph.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A context manager using this session as the default session.\n",
      " |  \n",
      " |  list_devices(self)\n",
      " |      Lists available devices in this session.\n",
      " |      \n",
      " |      ```python\n",
      " |      devices = sess.list_devices()\n",
      " |      for d in devices:\n",
      " |        print(d.name)\n",
      " |      ```\n",
      " |      \n",
      " |      Each element in the list has the following properties:\n",
      " |       - `name`: A string with the full name of the device. ex:\n",
      " |            `/job:worker/replica:0/task:3/device:CPU:0`\n",
      " |       - `device_type`: The type of the device (e.g. `CPU`, `GPU`, `TPU`.)\n",
      " |       - `memory_limit`: The maximum amount of memory available on the device.\n",
      " |            Note: depending on the device, it is possible the usable memory could\n",
      " |            be substantially less.\n",
      " |      Raises:\n",
      " |        tf.errors.OpError: If it encounters an error (e.g. session is in an\n",
      " |        invalid state, or network errors occur).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of devices in the session.\n",
      " |  \n",
      " |  make_callable(self, fetches, feed_list=None, accept_options=False)\n",
      " |      Returns a Python callable that runs a particular step.\n",
      " |      \n",
      " |      The returned callable will take `len(feed_list)` arguments whose types\n",
      " |      must be compatible feed values for the respective elements of `feed_list`.\n",
      " |      For example, if element `i` of `feed_list` is a `tf.Tensor`, the `i`th\n",
      " |      argument to the returned callable must be a numpy ndarray (or something\n",
      " |      convertible to an ndarray) with matching element type and shape. See\n",
      " |      `tf.Session.run` for details of the allowable feed key and value types.\n",
      " |      \n",
      " |      The returned callable will have the same return type as\n",
      " |      `tf.Session.run(fetches, ...)`. For example, if `fetches` is a `tf.Tensor`,\n",
      " |      the callable will return a numpy ndarray; if `fetches` is a `tf.Operation`,\n",
      " |      it will return `None`.\n",
      " |      \n",
      " |      Args:\n",
      " |        fetches: A value or list of values to fetch. See `tf.Session.run`\n",
      " |          for details of the allowable fetch types.\n",
      " |        feed_list: (Optional.) A list of `feed_dict` keys. See\n",
      " |          `tf.Session.run` for details of the allowable feed key types.\n",
      " |        accept_options: (Optional.) If `True`, the returned `Callable` will be\n",
      " |          able to accept `tf.RunOptions` and `tf.RunMetadata` as optional\n",
      " |          keyword arguments `options` and `run_metadata`, respectively, with\n",
      " |          the same syntax and semantics as `tf.Session.run`, which is useful\n",
      " |          for certain use cases (profiling and debugging) but will result in\n",
      " |          measurable slowdown of the `Callable`'s performance. Default: `False`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A function that when called will execute the step defined by\n",
      " |        `feed_list` and `fetches` in this session.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `fetches` or `feed_list` cannot be interpreted\n",
      " |          as arguments to `tf.Session.run`.\n",
      " |  \n",
      " |  partial_run(self, handle, fetches, feed_dict=None)\n",
      " |      Continues the execution with more feeds and fetches.\n",
      " |      \n",
      " |      This is EXPERIMENTAL and subject to change.\n",
      " |      \n",
      " |      To use partial execution, a user first calls `partial_run_setup()` and\n",
      " |      then a sequence of `partial_run()`. `partial_run_setup` specifies the\n",
      " |      list of feeds and fetches that will be used in the subsequent\n",
      " |      `partial_run` calls.\n",
      " |      \n",
      " |      The optional `feed_dict` argument allows the caller to override\n",
      " |      the value of tensors in the graph. See run() for more information.\n",
      " |      \n",
      " |      Below is a simple example:\n",
      " |      \n",
      " |      ```python\n",
      " |      a = array_ops.placeholder(dtypes.float32, shape=[])\n",
      " |      b = array_ops.placeholder(dtypes.float32, shape=[])\n",
      " |      c = array_ops.placeholder(dtypes.float32, shape=[])\n",
      " |      r1 = math_ops.add(a, b)\n",
      " |      r2 = math_ops.multiply(r1, c)\n",
      " |      \n",
      " |      h = sess.partial_run_setup([r1, r2], [a, b, c])\n",
      " |      res = sess.partial_run(h, r1, feed_dict={a: 1, b: 2})\n",
      " |      res = sess.partial_run(h, r2, feed_dict={c: res})\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        handle: A handle for a sequence of partial runs.\n",
      " |        fetches: A single graph element, a list of graph elements,\n",
      " |          or a dictionary whose values are graph elements or lists of graph\n",
      " |          elements (see documentation for `run`).\n",
      " |        feed_dict: A dictionary that maps graph elements to values\n",
      " |          (described above).\n",
      " |      \n",
      " |      Returns:\n",
      " |        Either a single value if `fetches` is a single graph element, or\n",
      " |        a list of values if `fetches` is a list, or a dictionary with the\n",
      " |        same keys as `fetches` if that is a dictionary\n",
      " |        (see documentation for `run`).\n",
      " |      \n",
      " |      Raises:\n",
      " |        tf.errors.OpError: Or one of its subclasses on error.\n",
      " |  \n",
      " |  partial_run_setup(self, fetches, feeds=None)\n",
      " |      Sets up a graph with feeds and fetches for partial run.\n",
      " |      \n",
      " |      This is EXPERIMENTAL and subject to change.\n",
      " |      \n",
      " |      Note that contrary to `run`, `feeds` only specifies the graph elements.\n",
      " |      The tensors will be supplied by the subsequent `partial_run` calls.\n",
      " |      \n",
      " |      Args:\n",
      " |        fetches: A single graph element, or a list of graph elements.\n",
      " |        feeds: A single graph element, or a list of graph elements.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A handle for partial run.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If this `Session` is in an invalid state (e.g. has been\n",
      " |          closed).\n",
      " |        TypeError: If `fetches` or `feed_dict` keys are of an inappropriate type.\n",
      " |        tf.errors.OpError: Or one of its subclasses if a TensorFlow error happens.\n",
      " |  \n",
      " |  run(self, fetches, feed_dict=None, options=None, run_metadata=None)\n",
      " |      Runs operations and evaluates tensors in `fetches`.\n",
      " |      \n",
      " |      This method runs one \"step\" of TensorFlow computation, by\n",
      " |      running the necessary graph fragment to execute every `Operation`\n",
      " |      and evaluate every `Tensor` in `fetches`, substituting the values in\n",
      " |      `feed_dict` for the corresponding input values.\n",
      " |      \n",
      " |      The `fetches` argument may be a single graph element, or an arbitrarily\n",
      " |      nested list, tuple, namedtuple, dict, or OrderedDict containing graph\n",
      " |      elements at its leaves.  A graph element can be one of the following types:\n",
      " |      \n",
      " |      * An `tf.Operation`.\n",
      " |        The corresponding fetched value will be `None`.\n",
      " |      * A `tf.Tensor`.\n",
      " |        The corresponding fetched value will be a numpy ndarray containing the\n",
      " |        value of that tensor.\n",
      " |      * A `tf.SparseTensor`.\n",
      " |        The corresponding fetched value will be a\n",
      " |        `tf.SparseTensorValue`\n",
      " |        containing the value of that sparse tensor.\n",
      " |      * A `get_tensor_handle` op.  The corresponding fetched value will be a\n",
      " |        numpy ndarray containing the handle of that tensor.\n",
      " |      * A `string` which is the name of a tensor or operation in the graph.\n",
      " |      \n",
      " |      The value returned by `run()` has the same shape as the `fetches` argument,\n",
      " |      where the leaves are replaced by the corresponding values returned by\n",
      " |      TensorFlow.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |         a = tf.constant([10, 20])\n",
      " |         b = tf.constant([1.0, 2.0])\n",
      " |         # 'fetches' can be a singleton\n",
      " |         v = session.run(a)\n",
      " |         # v is the numpy array [10, 20]\n",
      " |         # 'fetches' can be a list.\n",
      " |         v = session.run([a, b])\n",
      " |         # v is a Python list with 2 numpy arrays: the 1-D array [10, 20] and the\n",
      " |         # 1-D array [1.0, 2.0]\n",
      " |         # 'fetches' can be arbitrary lists, tuples, namedtuple, dicts:\n",
      " |         MyData = collections.namedtuple('MyData', ['a', 'b'])\n",
      " |         v = session.run({'k1': MyData(a, b), 'k2': [b, a]})\n",
      " |         # v is a dict with\n",
      " |         # v['k1'] is a MyData namedtuple with 'a' (the numpy array [10, 20]) and\n",
      " |         # 'b' (the numpy array [1.0, 2.0])\n",
      " |         # v['k2'] is a list with the numpy array [1.0, 2.0] and the numpy array\n",
      " |         # [10, 20].\n",
      " |      ```\n",
      " |      \n",
      " |      The optional `feed_dict` argument allows the caller to override\n",
      " |      the value of tensors in the graph. Each key in `feed_dict` can be\n",
      " |      one of the following types:\n",
      " |      \n",
      " |      * If the key is a `tf.Tensor`, the\n",
      " |        value may be a Python scalar, string, list, or numpy ndarray\n",
      " |        that can be converted to the same `dtype` as that\n",
      " |        tensor. Additionally, if the key is a\n",
      " |        `tf.placeholder`, the shape of\n",
      " |        the value will be checked for compatibility with the placeholder.\n",
      " |      * If the key is a\n",
      " |        `tf.SparseTensor`,\n",
      " |        the value should be a\n",
      " |        `tf.SparseTensorValue`.\n",
      " |      * If the key is a nested tuple of `Tensor`s or `SparseTensor`s, the value\n",
      " |        should be a nested tuple with the same structure that maps to their\n",
      " |        corresponding values as above.\n",
      " |      \n",
      " |      Each value in `feed_dict` must be convertible to a numpy array of the dtype\n",
      " |      of the corresponding key.\n",
      " |      \n",
      " |      The optional `options` argument expects a [`RunOptions`] proto. The options\n",
      " |      allow controlling the behavior of this particular step (e.g. turning tracing\n",
      " |      on).\n",
      " |      \n",
      " |      The optional `run_metadata` argument expects a [`RunMetadata`] proto. When\n",
      " |      appropriate, the non-Tensor output of this step will be collected there. For\n",
      " |      example, when users turn on tracing in `options`, the profiled info will be\n",
      " |      collected into this argument and passed back.\n",
      " |      \n",
      " |      Args:\n",
      " |        fetches: A single graph element, a list of graph elements,\n",
      " |          or a dictionary whose values are graph elements or lists of graph\n",
      " |          elements (described above).\n",
      " |        feed_dict: A dictionary that maps graph elements to values\n",
      " |          (described above).\n",
      " |        options: A [`RunOptions`] protocol buffer\n",
      " |        run_metadata: A [`RunMetadata`] protocol buffer\n",
      " |      \n",
      " |      Returns:\n",
      " |        Either a single value if `fetches` is a single graph element, or\n",
      " |        a list of values if `fetches` is a list, or a dictionary with the\n",
      " |        same keys as `fetches` if that is a dictionary (described above).\n",
      " |        Order in which `fetches` operations are evaluated inside the call\n",
      " |        is undefined.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If this `Session` is in an invalid state (e.g. has been\n",
      " |          closed).\n",
      " |        TypeError: If `fetches` or `feed_dict` keys are of an inappropriate type.\n",
      " |        ValueError: If `fetches` or `feed_dict` keys are invalid or refer to a\n",
      " |          `Tensor` that doesn't exist.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseSession:\n",
      " |  \n",
      " |  graph\n",
      " |      The graph that was launched in this session.\n",
      " |  \n",
      " |  graph_def\n",
      " |      A serializable version of the underlying TensorFlow graph.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A graph_pb2.GraphDef proto containing nodes for all of the Operations in\n",
      " |        the underlying TensorFlow graph.\n",
      " |  \n",
      " |  sess_str\n",
      " |      The TensorFlow process to which this session will connect.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from SessionInterface:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.InteractiveSession)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function placeholder in module tensorflow.python.ops.array_ops:\n",
      "\n",
      "placeholder(dtype, shape=None, name=None)\n",
      "    Inserts a placeholder for a tensor that will be always fed.\n",
      "    \n",
      "    **Important**: This tensor will produce an error if evaluated. Its value must\n",
      "    be fed using the `feed_dict` optional argument to `Session.run()`,\n",
      "    `Tensor.eval()`, or `Operation.run()`.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    ```python\n",
      "    x = tf.placeholder(tf.float32, shape=(1024, 1024))\n",
      "    y = tf.matmul(x, x)\n",
      "    \n",
      "    with tf.Session() as sess:\n",
      "      print(sess.run(y))  # ERROR: will fail because x was not fed.\n",
      "    \n",
      "      rand_array = np.random.rand(1024, 1024)\n",
      "      print(sess.run(y, feed_dict={x: rand_array}))  # Will succeed.\n",
      "    ```\n",
      "    \n",
      "    @compatibility(eager)\n",
      "    Placeholders are not compatible with eager execution.\n",
      "    @end_compatibility\n",
      "    \n",
      "    Args:\n",
      "      dtype: The type of elements in the tensor to be fed.\n",
      "      shape: The shape of the tensor to be fed (optional). If the shape is not\n",
      "        specified, you can feed a tensor of any shape.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor` that may be used as a handle for feeding a value, but not\n",
      "      evaluated directly.\n",
      "    \n",
      "    Raises:\n",
      "      RuntimeError: if eager execution is enabled\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.matmul(x,W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-59-78b4c0e0d208>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "  batch = mnist.train.next_batch(100)\n",
    "  train_step.run(feed_dict={x: batch[0], y_: batch[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9201\n"
     ]
    }
   ],
   "source": [
    "print(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, [-1,28,28,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.02\n",
      "step 100, training accuracy 0.92\n",
      "step 200, training accuracy 0.92\n",
      "step 300, training accuracy 0.92\n",
      "step 400, training accuracy 0.98\n",
      "step 500, training accuracy 0.98\n",
      "step 600, training accuracy 0.96\n",
      "step 700, training accuracy 0.94\n",
      "step 800, training accuracy 0.94\n",
      "step 900, training accuracy 0.94\n",
      "step 1000, training accuracy 0.98\n",
      "step 1100, training accuracy 0.96\n",
      "step 1200, training accuracy 0.96\n",
      "step 1300, training accuracy 0.94\n",
      "step 1400, training accuracy 0.96\n",
      "step 1500, training accuracy 1\n",
      "step 1600, training accuracy 0.98\n",
      "step 1700, training accuracy 1\n",
      "step 1800, training accuracy 1\n",
      "step 1900, training accuracy 0.96\n",
      "step 2000, training accuracy 0.98\n",
      "step 2100, training accuracy 1\n",
      "step 2200, training accuracy 0.98\n",
      "step 2300, training accuracy 1\n",
      "step 2400, training accuracy 1\n",
      "test accuracy 0.9798\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(2500):\n",
    "  batch = mnist.train.next_batch(50)\n",
    "  if i%100 == 0:\n",
    "    train_accuracy = accuracy.eval(feed_dict={\n",
    "        x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "    print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# KERAS\n",
    "\n",
    "The following is provided by: Ashok Tankala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 10, 10, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               102528    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 113,898\n",
      "Trainable params: 113,898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 52s 868us/step - loss: 0.3162 - acc: 0.9050 - val_loss: 0.0717 - val_acc: 0.9771\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 52s 861us/step - loss: 0.0762 - acc: 0.9766 - val_loss: 0.0492 - val_acc: 0.9833\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 52s 863us/step - loss: 0.0550 - acc: 0.9828 - val_loss: 0.0375 - val_acc: 0.9882\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 52s 860us/step - loss: 0.0444 - acc: 0.9859 - val_loss: 0.0306 - val_acc: 0.9894\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 52s 859us/step - loss: 0.0371 - acc: 0.9883 - val_loss: 0.0317 - val_acc: 0.9894\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 51s 855us/step - loss: 0.0318 - acc: 0.9899 - val_loss: 0.0286 - val_acc: 0.9909\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 51s 856us/step - loss: 0.0286 - acc: 0.9911 - val_loss: 0.0262 - val_acc: 0.9913\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 51s 855us/step - loss: 0.0238 - acc: 0.9923 - val_loss: 0.0279 - val_acc: 0.9906\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 52s 858us/step - loss: 0.0212 - acc: 0.9931 - val_loss: 0.0246 - val_acc: 0.9910\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 52s 862us/step - loss: 0.0193 - acc: 0.9935 - val_loss: 0.0260 - val_acc: 0.9917\n",
      "Metrics(Test loss & Test Accuracy): \n",
      "[0.025991347407873764, 0.9917]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshaping to format which CNN expects (batch, height, width, channels)\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1).astype('float32')\n",
    "\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train/=255\n",
    "X_test/=255\n",
    "\n",
    "# one hot encode\n",
    "number_of_classes = 10\n",
    "y_train = np_utils.to_categorical(y_train, number_of_classes)\n",
    "y_test = np_utils.to_categorical(y_test, number_of_classes)\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (5, 5), input_shape=(X_train.shape[1], X_train.shape[2], 1), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(number_of_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200)\n",
    "\n",
    "# Final evaluation of the model\n",
    "metrics = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Metrics(Test loss & Test Accuracy): \")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Conv2D in module keras.layers.convolutional:\n",
      "\n",
      "class Conv2D(_Conv)\n",
      " |  2D convolution layer (e.g. spatial convolution over images).\n",
      " |  \n",
      " |  This layer creates a convolution kernel that is convolved\n",
      " |  with the layer input to produce a tensor of\n",
      " |  outputs. If `use_bias` is True,\n",
      " |  a bias vector is created and added to the outputs. Finally, if\n",
      " |  `activation` is not `None`, it is applied to the outputs as well.\n",
      " |  \n",
      " |  When using this layer as the first layer in a model,\n",
      " |  provide the keyword argument `input_shape`\n",
      " |  (tuple of integers, does not include the sample axis),\n",
      " |  e.g. `input_shape=(128, 128, 3)` for 128x128 RGB pictures\n",
      " |  in `data_format=\"channels_last\"`.\n",
      " |  \n",
      " |  # Arguments\n",
      " |      filters: Integer, the dimensionality of the output space\n",
      " |          (i.e. the number of output filters in the convolution).\n",
      " |      kernel_size: An integer or tuple/list of 2 integers, specifying the\n",
      " |          height and width of the 2D convolution window.\n",
      " |          Can be a single integer to specify the same value for\n",
      " |          all spatial dimensions.\n",
      " |      strides: An integer or tuple/list of 2 integers,\n",
      " |          specifying the strides of the convolution\n",
      " |          along the height and width.\n",
      " |          Can be a single integer to specify the same value for\n",
      " |          all spatial dimensions.\n",
      " |          Specifying any stride value != 1 is incompatible with specifying\n",
      " |          any `dilation_rate` value != 1.\n",
      " |      padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n",
      " |          Note that `\"same\"` is slightly inconsistent across backends with\n",
      " |          `strides` != 1, as described\n",
      " |          [here](https://github.com/keras-team/keras/pull/9473#issuecomment-372166860)\n",
      " |      data_format: A string,\n",
      " |          one of `\"channels_last\"` or `\"channels_first\"`.\n",
      " |          The ordering of the dimensions in the inputs.\n",
      " |          `\"channels_last\"` corresponds to inputs with shape\n",
      " |          `(batch, height, width, channels)` while `\"channels_first\"`\n",
      " |          corresponds to inputs with shape\n",
      " |          `(batch, channels, height, width)`.\n",
      " |          It defaults to the `image_data_format` value found in your\n",
      " |          Keras config file at `~/.keras/keras.json`.\n",
      " |          If you never set it, then it will be \"channels_last\".\n",
      " |      dilation_rate: an integer or tuple/list of 2 integers, specifying\n",
      " |          the dilation rate to use for dilated convolution.\n",
      " |          Can be a single integer to specify the same value for\n",
      " |          all spatial dimensions.\n",
      " |          Currently, specifying any `dilation_rate` value != 1 is\n",
      " |          incompatible with specifying any stride value != 1.\n",
      " |      activation: Activation function to use\n",
      " |          (see [activations](../activations.md)).\n",
      " |          If you don't specify anything, no activation is applied\n",
      " |          (ie. \"linear\" activation: `a(x) = x`).\n",
      " |      use_bias: Boolean, whether the layer uses a bias vector.\n",
      " |      kernel_initializer: Initializer for the `kernel` weights matrix\n",
      " |          (see [initializers](../initializers.md)).\n",
      " |      bias_initializer: Initializer for the bias vector\n",
      " |          (see [initializers](../initializers.md)).\n",
      " |      kernel_regularizer: Regularizer function applied to\n",
      " |          the `kernel` weights matrix\n",
      " |          (see [regularizer](../regularizers.md)).\n",
      " |      bias_regularizer: Regularizer function applied to the bias vector\n",
      " |          (see [regularizer](../regularizers.md)).\n",
      " |      activity_regularizer: Regularizer function applied to\n",
      " |          the output of the layer (its \"activation\").\n",
      " |          (see [regularizer](../regularizers.md)).\n",
      " |      kernel_constraint: Constraint function applied to the kernel matrix\n",
      " |          (see [constraints](../constraints.md)).\n",
      " |      bias_constraint: Constraint function applied to the bias vector\n",
      " |          (see [constraints](../constraints.md)).\n",
      " |  \n",
      " |  # Input shape\n",
      " |      4D tensor with shape:\n",
      " |      `(batch, channels, rows, cols)`\n",
      " |      if `data_format` is `\"channels_first\"`\n",
      " |      or 4D tensor with shape:\n",
      " |      `(batch, rows, cols, channels)`\n",
      " |      if `data_format` is `\"channels_last\"`.\n",
      " |  \n",
      " |  # Output shape\n",
      " |      4D tensor with shape:\n",
      " |      `(batch, filters, new_rows, new_cols)`\n",
      " |      if `data_format` is `\"channels_first\"`\n",
      " |      or 4D tensor with shape:\n",
      " |      `(batch, new_rows, new_cols, filters)`\n",
      " |      if `data_format` is `\"channels_last\"`.\n",
      " |      `rows` and `cols` values might have changed due to padding.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Conv2D\n",
      " |      _Conv\n",
      " |      keras.engine.base_layer.Layer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      # Returns\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _Conv:\n",
      " |  \n",
      " |  build(self, input_shape)\n",
      " |      Creates the layer weights.\n",
      " |      \n",
      " |      Must be implemented on all layers that have weights.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          input_shape: Keras tensor (future input to layer)\n",
      " |              or list/tuple of Keras tensors to reference\n",
      " |              for weight shape computations.\n",
      " |  \n",
      " |  call(self, inputs)\n",
      " |      This is where the layer's logic lives.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: Input tensor, or list/tuple of input tensors.\n",
      " |          **kwargs: Additional keyword arguments.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A tensor or list/tuple of tensors.\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      Assumes that the layer will be built\n",
      " |      to match that input shape provided.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          An input shape tuple.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __call__(self, inputs, **kwargs)\n",
      " |      Wrapper around self.call(), for handling internal references.\n",
      " |      \n",
      " |      If a Keras tensor is passed:\n",
      " |          - We call self._add_inbound_node().\n",
      " |          - If necessary, we `build` the layer to match\n",
      " |              the _keras_shape of the input(s).\n",
      " |          - We update the _keras_shape of every input tensor with\n",
      " |              its new shape (obtained via self.compute_output_shape).\n",
      " |              This is done as part of _add_inbound_node().\n",
      " |          - We update the _keras_history of the output tensor(s)\n",
      " |              with the current layer.\n",
      " |              This is done as part of _add_inbound_node().\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: Can be a tensor or list/tuple of tensors.\n",
      " |          **kwargs: Additional keyword arguments to be passed to `call()`.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output of the layer's `call` method.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: in case the layer is missing shape information\n",
      " |              for its `build` call.\n",
      " |  \n",
      " |  add_loss(self, losses, inputs=None)\n",
      " |      Adds losses to the layer.\n",
      " |      \n",
      " |      The loss may potentially be conditional on some inputs tensors,\n",
      " |      for instance activity losses are conditional on the layer's inputs.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          losses: loss tensor or list of loss tensors\n",
      " |              to add to the layer.\n",
      " |          inputs: input tensor or list of inputs tensors to mark\n",
      " |              the losses as conditional on these inputs.\n",
      " |              If None is passed, the loss is assumed unconditional\n",
      " |              (e.g. L2 weight regularization, which only depends\n",
      " |              on the layer's weights variables, not on any inputs tensors).\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Adds updates to the layer.\n",
      " |      \n",
      " |      The updates may potentially be conditional on some inputs tensors,\n",
      " |      for instance batch norm updates are conditional on the layer's inputs.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          updates: update op or list of update ops\n",
      " |              to add to the layer.\n",
      " |          inputs: input tensor or list of inputs tensors to mark\n",
      " |              the updates as conditional on these inputs.\n",
      " |              If None is passed, the updates are assumed unconditional.\n",
      " |  \n",
      " |  add_weight(self, name, shape, dtype=None, initializer=None, regularizer=None, trainable=True, constraint=None)\n",
      " |      Adds a weight variable to the layer.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          name: String, the name for the weight variable.\n",
      " |          shape: The shape tuple of the weight.\n",
      " |          dtype: The dtype of the weight.\n",
      " |          initializer: An Initializer instance (callable).\n",
      " |          regularizer: An optional Regularizer instance.\n",
      " |          trainable: A boolean, whether the weight should\n",
      " |              be trained via backprop or not (assuming\n",
      " |              that the layer itself is also trainable).\n",
      " |          constraint: An optional Constraint instance.\n",
      " |      \n",
      " |      # Returns\n",
      " |          The created weight variable.\n",
      " |  \n",
      " |  assert_input_compatibility(self, inputs)\n",
      " |      Checks compatibility between the layer and provided inputs.\n",
      " |      \n",
      " |      This checks that the tensor(s) `input`\n",
      " |      verify the input assumptions of the layer\n",
      " |      (if any). If not, exceptions are raised.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: input tensor or list of input tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: in case of mismatch between\n",
      " |              the provided inputs and the expectations of the layer.\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask=None)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      # Returns\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Counts the total number of scalars composing the weights.\n",
      " |      \n",
      " |      # Returns\n",
      " |          An integer count.\n",
      " |      \n",
      " |      # Raises\n",
      " |          RuntimeError: if the layer isn't yet built\n",
      " |              (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Weights values as a list of numpy arrays.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from Numpy arrays.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          weights: a list of Numpy arrays. The number\n",
      " |              of arrays and their shape must match\n",
      " |              number of the dimensions of the weights\n",
      " |              of the layer (i.e. it should match the\n",
      " |              output of `get_weights`).\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: If the provided weights list does not match the\n",
      " |              layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates a layer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same layer from the config\n",
      " |      dictionary. It does not handle layer connectivity\n",
      " |      (handled by Network), nor weights (handled by `set_weights`).\n",
      " |      \n",
      " |      # Arguments\n",
      " |          config: A Python dictionary, typically the\n",
      " |              output of get_config.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  built\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape tuple(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Input shape tuple\n",
      " |          (or list of input shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  losses\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output tensor or list of output tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape tuple(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one inbound node,\n",
      " |      or if all inbound nodes have the same output shape.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output shape tuple\n",
      " |          (or list of input shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  trainable_weights\n",
      " |  \n",
      " |  updates\n",
      " |  \n",
      " |  weights\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Conv2D)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
